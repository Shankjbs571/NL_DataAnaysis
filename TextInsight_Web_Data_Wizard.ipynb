{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1M1PRG5MjfWobK-rczTi6Zt8Yo2FCoO2r",
      "authorship_tag": "ABX9TyO7/WZu9ixHkPYF5JGe996q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shankjbs571/TextInsight-Web-Data-Wizard/blob/main/TextInsight_Web_Data_Wizard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x0q_Mlp_hA-w"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/Data/Output Data Structure.xlsx\")"
      ],
      "metadata": {
        "id": "NY5cd3B_hY5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check whats inside df\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hPpq1Zx_kiMv",
        "outputId": "73a6fc02-44c2-49a8-b492-d86d50a4a0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(df['URL'][16])\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "soup"
      ],
      "metadata": {
        "id": "nxoXZD7QkwPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = soup.find('h1', class_='entry-title').text\n",
        "title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YqyUy5ielhZ_",
        "outputId": "5fa34638-5acb-43c2-aa29-d7bd2b7ab300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rise of Chatbots and its impact on customer support by the year 2040'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = soup.find('div', class_='td-post-content tagdiv-type').text\n",
        "article"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "8cgTbty6l8qD",
        "outputId": "3bb954c6-8c65-4e10-afb2-0001645ca35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe human race is known to come up with inventions that decide its future. Many scholars believe that Artificial Intelligence(AI) has been the most pioneering innovation since early humans discovered fire and invented the wheel. The advancement of groundbreaking technologies such as Artificial Intelligence and Machine Learning (ML) have constantly been shaping the future of humankind. The likes of Mark Zuckerberg have welcomed the evolution of AI. He once stated- “With A.I. especially, I am optimistic“. While according to Elon Musk, “AI can outsmart and endanger us.” However, we simply cannot ignore how useful and influential AI is in our daily lives.\\nThe applications of AI and ML have delved deeply into various domains. The least to mention are Healthcare, Transportation, Education, Media, Communication, Manufacturing, and Customer service. Businesses worldwide are placing a significant emphasis on enhancing their customers’ overall experiences. As a result of AI, a massive leap in customer experience service has come up with the evolution of chatbots.\\nWhat is a chatbot, and how is it useful in business?\\nChatbots are AI-based systems used by businesses for communication with customers. Customers can interact with chatbots directly via chat and voice applications. Numerous online customer service platforms are utilizing chatbots to manage bulk phone calls, FAQs, and other common customer service inquiries. Chatbots resolve customer queries within no time and are accessible 24X7, enhancing the customer experience.\\nThe use of technologies such as AI, ML, and NLP (Natural Language Processing), enables the bot to analyze and identify the user’s problems and instantly give the most relevant answers to the queries.\\nPlatforms such as Facebook Messenger, WeChat, Discord, and Slack have been extensively using chatbots for better communication.\\nAdvantages of using chatbots and their impact on customer support\\nThere are numerous advantages of using chatbots in businesses- Chatbots can communicate with multiple customers at once. They can be used to send regular updates about the products and exclusive offers by the company. They are available throughout the day, which is crucial for businesses that expand globally in different time zones. It becomes too cumbersome for businesses to have customer care support centers at various locations across the world. There is no human intervention involved. Therefore, chatbots can be accessed worldwide anytime. Chatbots reply instantly, and hence it increases customer engagement. Chatbots can also be extensively used to collect user data and analyze it further to provide customized solutions. ML-based chatbot systems are learning from past conversations to generate more thoughtful replies.\\nRoughly 40 % of internet users around the globe prefer interacting with chatbots rather than virtual agents. With leading sectors such as retail and healthcare rapidly turning towards digital technology, chatbots will continue to grow in popularity in the years to come.\\nChatbot Market Trends and forecast about its impact on customer support by 2040\\nAccording to some reports, it is anticipated that the size of the global chatbot market will expand at a compound annual growth rate (CAGR) of approximately 26% from the years 2022 to 2030. In the year 2021, the chatbot market size was valued at a total of USD 525 million.\\nIt is anticipated that the growing adoption of customer service activities among enterprises to reduce operating costs will be the primary factor driving the chatbot market.\\nIn the past few years, some of the most significant advances that have been made in artificial intelligence and machine learning have been in the areas of perception and cognition. Although there is much room for improvement in voice recognition technology, the tech giants like Apple, Amazon, and Google have all made significant strides in this direction with their digital assistants Siri, Alexa, and Google Assistant, respectively.\\nDeep learning systems are now actively utilized to decide trades accomplished on Wall Street and predict whether users will click on particular advertisements. It has increased efficiency on both the consumer and the business side of things. Chatbots can help businesses save money by generating revenue, conducting research, generating leads, and increasing brand awareness. Other uses for chatbots include research and lead generation.\\nGiven the trends of the data, we can expect the chatbot industry to surpass the billion-dollar mark in a few years. Hence, by 2040 the chatbot industry would be having a multi-billion dollar market value.\\nHow are companies utilizing chatbots to provide superior support for their customers- A comprehensive analysis that provides deep insights into the future of customer service in 2040.\\nBank of America is responsible for creating one of the most groundbreaking innovations. The recently introduced “Erica” is intended to assist customers in making more informed choices regarding their finances. Customers can make payments, check their balances, save money, and transfer funds with this chatbot, which uses artificial intelligence, algorithms, and predictive messaging. Erica makes it possible for users to interact via text and voice, and it then uses the information provided to push insights to the customer.\\nKLM Royal Dutch Airlines is another company incorporating chatbots into their customer service channel. KLM increased the number of customer interactions with the Facebook messenger app by forty percent. The airline wanted to facilitate more effortless conversations between its employees and passengers without compromising passengers’ right to personal privacy. KLM was aware that its customers spent a significant amount of time using Facebook and Messenger, so the company decided to implement a chatbot as its platform for customer service. Messenger is now the channel through which customers can directly receive booking details, boarding passes, and flight status updates. On a typical day, KLM receives an average of five messages per minute through Facebook Messenger. During peak traffic, that number jumps to over eleven messages per minute.\\nThese innovations with chatbots will surely pave the way for the future. by 2040, chatbots will completely revolutionize the customer experience domain.\\nChallenges ahead\\nThere are apparent constraints with AI technology. The absence of emotional intelligence restricts the range and depth of the conversation that can take place when AI-driven chatbots are used with customers, even though the chatbots can learn from and predict the behavior of the customers.\\nThe availability of trained staff and their familiarity with this developing technology is another factor that can hinder the effectiveness of chatbots. Chatbots can have significant limitations when it comes to different languages and accents. This restriction turns into a significant obstacle for companies that operate in more than one location. Maintaining the security of the customer’s data is also a key challenge.\\nConclusion\\nEven though chatbot technology has some limitations in how it can function, there are innumerable advantages of using this technology in improving customer support. The chatbot technology is potent enough to enhance customer support multi-folds by 2040. The optimistic thing about this technology is that ML-based systems can learn from their own experiences and improve with time. Hence there is a high chance that chatbot technology is perhaps “The next big thing” in the customer service domain.\\nBlackcoffer Insights 46: Faizan Raza, B.Tech from IIT Delhi\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape article title and text\n",
        "def scrape_title_article(link):\n",
        "    response = requests.get(link)\n",
        "    # print(response.status_code)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    #title\n",
        "    # title = soup.find('h1', class_='entry-title').text\n",
        "    # if title\n",
        "    # #article\n",
        "    # article = soup.find('div', class_='td-post-content tagdiv-type').text\n",
        "\n",
        "    if soup.find('div', class_='td-post-content') and soup.find('h1', class_='entry-title'):\n",
        "        title = soup.find('h1', class_='entry-title').text\n",
        "        article = soup.find('div', class_='td-post-content').text\n",
        "    elif soup.find('div', class_='td-post-content') and soup.find('h1', class_='tdb-title-text'):\n",
        "        title = soup.find('h1', class_='tdb-title-text').text\n",
        "        article = soup.find('div', class_='td-post-content').text\n",
        "    elif response.status_code == 404:\n",
        "        title = 'response.status_code == 404'\n",
        "        article = 'content was not found'\n",
        "    return title, article"
      ],
      "metadata": {
        "id": "JJ8feaTLnWrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scrape_title_article(df['URL'][35]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E0R4sqSvkWe",
        "outputId": "4f8a252e-f878-419e-d128-fd5ec5ca6076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "404\n",
            "('response.status_code == 404', 'content was not found')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_txt_and_save(filename, title, article):\n",
        "  output_dir=\"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/Articles_txt_files\"\n",
        "  filename = f\"{filename}.txt\"\n",
        "  file_path = os.path.join(output_dir, filename)\n",
        "  with open(file_path, 'w') as file:\n",
        "      file.write(f\"{title}\\n\\n\")\n",
        "      file.write(article)"
      ],
      "metadata": {
        "id": "tcLx8K2Oop2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_txt_and_save(\"Sample\",\"Title\",\"asdsjkdasdjfasbdjfbsadjh\")"
      ],
      "metadata": {
        "id": "8OHV5FXZrLDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for url_id, url in zip(df['URL_ID'],df['URL']):\n",
        "  print(url_id,url)\n",
        "  filename = url_id\n",
        "\n",
        "  #scrape the web\n",
        "  title,article = scrape_title_article(url)\n",
        "\n",
        "  #write and save the text file\n",
        "  create_txt_and_save(filename,title,article)\n",
        "  print(f\"file {filename} is created successfully\")"
      ],
      "metadata": {
        "id": "yM6wIZ9Bp99A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Now Lets Do the Analysis\n",
        "\n",
        "# Read the content of article.txt into a variable\n",
        "articlefile = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/Articles_txt_files/blackassign0001.txt\"\n",
        "with open(articlefile, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    # Join lines starting from the second line\n",
        "    article_content = ''.join(lines[1:])\n",
        "\n",
        "\n",
        "# Initialize a set to store stop words\n",
        "stop_words = set()\n",
        "\n",
        "# Read each stop word file and add words to the set\n",
        "stopword1 = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/StopWords/StopWords_Auditor.txt\"\n",
        "stopword2 = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/StopWords/StopWords_Currencies.txt\"\n",
        "stopword3 = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/StopWords/StopWords_DatesandNumbers.txt\"\n",
        "stopword4 = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/StopWords/StopWords_Generic.txt\"\n",
        "stopword5 = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/StopWords/StopWords_GenericLong.txt\"\n",
        "stopword6 = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/StopWords/StopWords_Geographic.txt\"\n",
        "stopword7 = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/StopWords/StopWords_Names.txt\"\n",
        "\n",
        "stopword_files = [stopword1, stopword2,stopword3,stopword4,stopword5,stopword6,stopword7 ]\n",
        "for stopword_file in stopword_files:\n",
        "  try:\n",
        "    with open(stopword_file, 'r',encoding='utf-8') as file:\n",
        "        stop_words.update(file.read().splitlines())\n",
        "  except UnicodeDecodeError:\n",
        "    with open(stopword_file, 'r', encoding='latin1') as file:\n",
        "        stop_words.update(file.read().splitlines())\n",
        "\n",
        "# Tokenize the article content\n",
        "words = article_content.split()\n",
        "\n",
        "# Remove stop words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Join the filtered words back into a string\n",
        "filtered_content = ' '.join(filtered_words)\n",
        "\n",
        "# Print or use the filtered content as needed\n",
        "print(len(words))\n",
        "print(len(filtered_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53ZgaHEW-_gs",
        "outputId": "7cb90762-d31f-4f07-ed9b-f3de886e4c1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1221\n",
            "575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq4NK27rlMcV",
        "outputId": "bf25629d-0088-4e21-84e6-97ccde210544"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example sentence with some stopwords that we want to remove.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(article_content)\n",
        "\n",
        "# Get the English stopwords\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from the text\n",
        "filtered_words = [word for word in words if word.lower() not in english_stopwords]\n",
        "\n",
        "# Join the filtered words back into a sentence\n",
        "filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "print(\"Original text:\", len(article_content))\n",
        "print(\"Text after removing stopwords:\", len(filtered_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xCCXB8rk9Mj",
        "outputId": "899aa537-2970-4617-ffaf-4cf8840716e5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: 6984\n",
            "Text after removing stopwords: 4983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set of Negative and Positive words\n",
        "# Load positive and negative words into sets\n",
        "pw = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/MasterDictionary/positive-words.txt\"\n",
        "nw = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/20211030 Test Assignment/MasterDictionary/negative-words.txt\"\n",
        "positive_words = set(open(pw).read().splitlines())\n",
        "try:\n",
        "  negative_words = set(open(nw,encoding=\"utf-8\").read().splitlines())\n",
        "except UnicodeDecodeError:\n",
        "  negative_words = set(open(nw,encoding='latin1').read().splitlines())\n",
        "\n",
        "print(len(positive_words))\n",
        "print(len(negative_words))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNsCJWC2TMn7",
        "outputId": "a2899dd1-7490-4b51-f917-cc03aa96e25c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2006\n",
            "4783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get the stopwords removed\n",
        "\n",
        "def remove_stop_words(article_content):\n",
        "  # Tokenize the article content\n",
        "  words = article_content.split()\n",
        "\n",
        "  # Remove stop words\n",
        "  filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "  # Join the filtered words back into a string\n",
        "  filtered_content = ' '.join(filtered_words)\n",
        "\n",
        "  return filtered_words,filtered_content"
      ],
      "metadata": {
        "id": "QDtKr0UwG9w5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title p_n_scores, polarity_score,subjectivity_score functions\n",
        "def p_n_scores(filtered_content):\n",
        "  filtered_content_words = filtered_content.split()\n",
        "  positive_score = 0\n",
        "  negative_score = 0\n",
        "  for word in filtered_content_words:\n",
        "    if word in positive_words:\n",
        "      positive_score+=1\n",
        "    elif word in negative_words:\n",
        "      negative_score+=1\n",
        "  return positive_score,negative_score\n",
        "\n",
        "def polarity_score(p,n):\n",
        "  diff = p-n\n",
        "  sum = p+n\n",
        "  sum+=0.000001\n",
        "  pol_score = diff/sum\n",
        "  return pol_score\n",
        "\n",
        "def subjectivity_score(p,n,total_words):\n",
        "  sum = p+n\n",
        "  total_words+=0.000001\n",
        "  return sum/total_words"
      ],
      "metadata": {
        "id": "QMAH-RdbEP7N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity_score(30,5,len(filtered_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8EY2F9jJPL1",
        "outputId": "798b86f9-97b2-4340-fda8-240e11e4a68c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.060869565111531194"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('cmudict')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSNfurdIUl3r",
        "outputId": "390e47fe-d2b2-43ab-8a73-e6bb2a723f50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lets Find How many Complex words are there\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "def count_syllables(word, pronunciations):\n",
        "    # Lookup the pronunciation of the word\n",
        "    word_pronunciations = pronunciations.get(word.lower(), [])\n",
        "    # word_pronunciations = pronunciations[word.lower()]\n",
        "    max_syllables = 0\n",
        "\n",
        "    # Iterate over each pronunciation\n",
        "    for pron in word_pronunciations:\n",
        "        syllable_count = 0\n",
        "        # Iterate over each syllable in the pronunciation\n",
        "        for syl in pron:\n",
        "            # Check if the syllable ends with a digit\n",
        "            if syl[-1].isdigit():\n",
        "                syllable_count += 1\n",
        "        # Update the maximum syllable count\n",
        "        if syllable_count > max_syllables:\n",
        "            max_syllables = syllable_count\n",
        "\n",
        "    return max_syllables\n",
        "\n",
        "\n",
        "def is_complex(word, pronunciations):\n",
        "    return count_syllables(word, pronunciations) > 2\n",
        "\n",
        "# Load the CMU Pronouncing Dictionary\n",
        "pronunciations = cmudict.dict()\n",
        "\n",
        "text = \"This is a sample text with complex words like calculation and determination.\"\n",
        "\n",
        "def complex_words_percentage(filtered_content,pronunciations):\n",
        "  # Tokenize the text into words\n",
        "  raw_words = word_tokenize(filtered_content)\n",
        "  words = [word for word in raw_words if re.match('^[a-zA-Z]+$', word)]\n",
        "\n",
        "  # Count the total number of words and complex words\n",
        "  total_words = len(words)\n",
        "  complex_words = sum([1 for word in words if is_complex(word, pronunciations)])\n",
        "\n",
        "  # Calculate the percentage of complex words\n",
        "  percentage_complex_words = (complex_words / total_words)*100\n",
        "  return total_words,complex_words,percentage_complex_words\n",
        "\n",
        "\n",
        "total_words,complex_words,percentage_complex_words = complex_words_percentage(filtered_content,pronunciations)\n",
        "print(f\"Total Words: {total_words}\")\n",
        "print(f\"Complex Words: {complex_words}\")\n",
        "print(f\"Percentage of Complex Words: {percentage_complex_words:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h73TBeHmTINh",
        "outputId": "17fcce93-550d-4679-fe7d-95b9d7472f52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 552\n",
            "Complex Words: 151\n",
            "Percentage of Complex Words: 27.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hUwqr19rdHVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analysis of Readibility\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "\n",
        "def fog_index(content,pronunciations):\n",
        "  # Tokenize the text into sentences\n",
        "  sentences = sent_tokenize(content)\n",
        "\n",
        "  # Count the number of sentences\n",
        "  num_sentences = len(sentences)\n",
        "  # total_words,complex_words,complex_words_p = complex_words_percentage(content,pronunciations)\n",
        "  total_words,complex_words,percentage_complex_words = complex_words_percentage(content,pronunciations)\n",
        "  avg_sent_len = total_words / num_sentences\n",
        "  # Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
        "  fog_i = 0.4 * (avg_sent_len + percentage_complex_words)\n",
        "  return fog_i , avg_sent_len,num_sentences\n",
        "\n",
        "fog_index(filtered_content,pronunciations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35L1KllDaLKx",
        "outputId": "33ad6e98-066f-4881-e569-690d5a154176"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13.886028985507247, 7.36, 75)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Syllable Count Per Word\n",
        "# Tokenize the text into words\n",
        "\n",
        "def syllable_count_per_word(article,):\n",
        "  words = word_tokenize(article)\n",
        "\n",
        "  # Calculate the total number of syllables and words\n",
        "  total_syllables = sum(count_syllables(word,pronunciations) for word in words)\n",
        "  total_words = len(words)\n",
        "\n",
        "  # Calculate the syllable count per word\n",
        "  if total_words > 0:\n",
        "      syllables_per_word = total_syllables / total_words\n",
        "  else:\n",
        "      syllables_per_word = 0\n",
        "\n",
        "  return total_syllables,total_words,syllables_per_word\n",
        "\n",
        "total_syllables,total_words,syllables_per_word = syllable_count_per_word(filtered_content)\n",
        "print(\"Total Syllables:\", total_syllables)\n",
        "print(\"Total Words:\", total_words)\n",
        "print(\"Syllable Count Per Word:\", syllables_per_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojz3bfxHLR6j",
        "outputId": "3ceea97e-06e9-418b-9f29-b8e65d6f1d5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Syllables: 1116\n",
            "Total Words: 721\n",
            "Syllable Count Per Word: 1.5478502080443828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Count Personal Pronouns\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    # Define the regex pattern to match the personal pronouns\n",
        "    pattern = r\"\\b(?:I|we|my|ours|us)\\b\"\n",
        "\n",
        "    # Use findall to get all matches of the pattern in the text\n",
        "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Filter out 'US' as a country name\n",
        "    filtered_matches = [match for match in matches if match.lower() != 'us']\n",
        "\n",
        "    # Return the count of personal pronouns\n",
        "    return len(filtered_matches)\n",
        "\n",
        "\n",
        "# Count personal pronouns in the text\n",
        "pronoun_count = count_personal_pronouns(article_content)\n",
        "\n",
        "print(\"Personal pronouns count:\", pronoun_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmD9TfWGNDXh",
        "outputId": "61f18d33-e847-4451-c1c7-fb16ff543a80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Personal pronouns count: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Average Word Length\n",
        "\n",
        "def average_word_length(text):\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Calculate the total number of characters in all words\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "\n",
        "    # Calculate the total number of words\n",
        "    total_words = len(words)\n",
        "\n",
        "    # Calculate the average word length\n",
        "    if total_words > 0:\n",
        "        average_length = total_characters / total_words\n",
        "    else:\n",
        "        average_length = 0\n",
        "\n",
        "    return average_length\n",
        "\n",
        "avg_length = average_word_length(filtered_content)\n",
        "\n",
        "print(\"Average Word Length:\", avg_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilhEbYAyN23k",
        "outputId": "bbc4bff2-d91b-4759-bf53-10746cbb85f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Word Length: 6.75304347826087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lets Write Data Back to excel sheet\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Data/Output.xlsx')\n",
        "path_to_txt = \"/content/drive/MyDrive/Colab Notebooks/Data/BlackcofferDataExtraction_Analysis/Articles_txt_files/\"\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    file_name = row['URL_ID']\n",
        "    text_path = os.path.join(path_to_txt, file_name + '.txt')  # Adjust the path as needed\n",
        "    with open(text_path, 'r') as file:\n",
        "      lines = file.readlines()\n",
        "      # Join lines starting from the second line\n",
        "      article_con = ''.join(lines[1:])\n",
        "\n",
        "    #get the filtered content\n",
        "    filtered_wor,filtered_con = remove_stop_words(article_con)\n",
        "\n",
        "    # Perform the analysis\n",
        "    positive_score,negative_score = p_n_scores(filtered_con)\n",
        "\n",
        "    pol_score = polarity_score(positive_score,negative_score)\n",
        "\n",
        "    sub_score = subjectivity_score(positive_score,negative_score,len(filtered_wor))\n",
        "\n",
        "    fg_index,avg_s_len,num_sentences = fog_index(filtered_con,pronunciations)\n",
        "\n",
        "    total_words,complex_words,percentage_complex_words = complex_words_percentage(filtered_con,pronunciations)\n",
        "\n",
        "    avg_num_words_per_sen = total_words/num_sentences\n",
        "\n",
        "    avg_length_word = average_word_length(filtered_con)\n",
        "\n",
        "    total_syllables,total_words,syllables_per_word = syllable_count_per_word(filtered_con)\n",
        "\n",
        "    pronoun_count = count_personal_pronouns(article_con)\n",
        "\n",
        "    avg_length = average_word_length(filtered_con)\n",
        "\n",
        "\n",
        "\n",
        "    # Now Updating the df with the analysis results\n",
        "    df.at[index, 'POSITIVE SCORE'] = positive_score\n",
        "    df.at[index, 'NEGATIVE SCORE'] = negative_score\n",
        "    df.at[index, 'POLARITY SCORE'] = pol_score\n",
        "    df.at[index, 'SUBJECTIVITY SCORE'] = sub_score\n",
        "    df.at[index, 'AVG SENTENCE LENGTH'] = avg_s_len\n",
        "    df.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n",
        "    df.at[index, 'FOG INDEX'] = fg_index\n",
        "    df.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = avg_num_words_per_sen\n",
        "    df.at[index, 'COMPLEX WORD COUNT'] = complex_words\n",
        "    df.at[index, 'WORD COUNT'] =  total_words\n",
        "    df.at[index, 'SYLLABLE PER WORD'] = syllables_per_word\n",
        "    df.at[index, 'PERSONAL PRONOUNS'] = pronoun_count\n",
        "    df.at[index, 'AVG WORD LENGTH'] = avg_length\n",
        "\n",
        "\n",
        "# Write the updated DataFrame back to the Excel file\n",
        "df.to_excel('/content/drive/MyDrive/Colab Notebooks/Data/Output.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "KCpqcpfGPHg5"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}